3.2 Case Study 3 – BAnQ
3.2.1 Context and Challenge
In 2019, Bibliothèque et Archives nationales du Québec (BAnQ) received a major archival donation from Groupe Cossette Communication, a prominent force in Canada’s advertising industry. The fonds was notable not only for its scale but for its hybrid composition, encompassing analog, textual, iconographic, audiovisual, and digital materials. Among the most substantial components were 25 linear metres of textual records, more than 23,000 photographic documents, approximately 1,000 audiovisual carriers, a single hard drive, and an exceptional total of 6,297 optical discs (CDs and DVDs). From these discs alone, 1.5 million digital files were extracted, with more than 2,200 discs requiring cleaning prior to processing.

The absence of a shared production schema or centralized metadata, coupled with the fact that this was an unprecedented type of donation for BAnQ, presented immediate challenges in terms of triage, appraisal, and preservation. The files consisted largely of final deliverables—commercials and campaign materials—but offered minimal insight into the creative or technical processes behind them. Further complexity arose from the multi-site origin of the records: various Cossette offices contributed content, each with its own workflows, naming conventions, and file structures.

3.2.2 Goals and Constraints
BAnQ established a multifaceted set of goals aimed at regaining intellectual and technical control over the collection. These included ensuring the secure extraction and inventory of digital files; performing selective appraisal to reduce volume while retaining representative material; establishing and maintaining traceability between digital objects and their physical counterparts; and meeting donor expectations related to accessibility, confidentiality, and financial evaluation.

The project was constrained by several material and contextual factors. On the technical side, data extraction from aging optical media proved slow and delicate, with legacy Mac-based formats complicating access and interpretation. The onset of the COVID-19 pandemic further disrupted physical workflows, delaying disc handling and onsite collaboration. Intellectually, the absence of a unifying file architecture or contextual documentation necessitated a case-by-case interpretive approach. In certain instances, access restrictions imposed by the donor further limited options for dissemination and description.

3.2.3 Protocol or Solution Developed
In response, BAnQ adopted an iterative, collaborative methodology grounded in experimentation and internal capacity building. At the outset, staff implemented automated workflows for file extraction, checksum verification, format validation, and metadata capture. A robotic cleaning system was introduced to expedite and standardize disc processing. Custom scripts were developed to flag redundancies, identify corrupted files, and diagnose structural anomalies such as excessive file path lengths or nested folders.

A comprehensive format inventory was compiled, and a set of triage criteria was developed to assess completeness, coherence, and potential research value. These steps were supported by decision trees, documentation protocols, and version control mechanisms. The work was carried out by a cross-functional team spanning the archival processing, digitization, and IT departments, fostering a culture of knowledge exchange and shared problem-solving. The toolkit of internal resources generated through this process became a transferable asset for future hybrid collections.

3.2.4 Implementation and Results
This structured yet adaptive approach yielded tangible results. Tens of thousands of files were processed, and significant reductions in digital volume were achieved through automated and manual appraisal. One project area, identified internally as “Digital Project 18,” initially encompassed nearly 30,000 files and 239 gigabytes of data. Following a targeted triage and reorganization process, this subset was distilled to a smaller, coherent body of representative materials.

Extraction protocols proved robust, even in cases where disc integrity was compromised. File inspection and content analysis allowed the team to prioritize materials of enduring value, while maintaining a clear record of treatments applied. Traceability between physical carriers and digital surrogates was preserved through meticulous documentation. The team also succeeded in integrating archival, technical, and financial workflows, ensuring that items appraised for monetary value could be retained in accordance with legal and institutional requirements.

3.2.5 Lessons Learned
The Cossette collection offered BAnQ a critical opportunity to refine its strategies for managing large-scale, hybrid-born donations. Chief among the lessons learned was the necessity of flexible protocols that combine human expertise with scalable automation. The project highlighted the archival importance of tools developed in-house—such as format inventories, metadata reports, and decision trees—that enable institutions to adapt to the idiosyncrasies of each collection.

Equally, it underscored the centrality of intellectual appraisal in digital preservation, beyond technical conformance alone. In the absence of formal metadata or consistent structure, interpretive work by archivists remains essential to reconstruct meaning and value. Donor relations also emerged as a key factor, influencing access policies, evaluation procedures, and retention obligations. Finally, the project reaffirmed the importance of cross-disciplinary coordination and iterative planning as cornerstones of contemporary archival practice.

3.2.6 Strategic Retention as a Transferable Practice
The Cossette collection case study offers more than a narrative of successful intervention; it provides the foundation for a broadly applicable model of strategic retention. Faced with an overwhelming volume of unstructured and inconsistently described digital files—most originating from legacy optical media and absent a coherent metadata framework—BAnQ was compelled to make a series of difficult decisions about what to keep, what to discard, and what to prioritize for further analysis or treatment. This necessity, born of scale and constraint, revealed a set of principles that may be adapted by other institutions confronting similarly complex digital acquisitions.

Rather than attempting to define universal technical thresholds or fixed rules for retention, this section presents a strategic retention framework—a decision-making template distilled from the Cossette case. Its purpose is not to prescribe actions, but to guide institutions in constructing their own appraisal logic based on clearly articulated factors: collection context, institutional priorities, resource limitations, and archival values.

Strategic Retention Framework (Derived from the Cossette Case)
1. Clarify Collection Context and Preservation Intent
Begin by defining the nature and origin of the collection. Is it composed of final products, working files, or both? Is its long-term value primarily evidential, historical, or artistic? What is known (or missing) about its provenance and production? Establish the institutional purpose of preservation in this context: is the goal comprehensive documentation, representativity, public access, or legal retention?

2. Generate a Technical and Descriptive Inventory
Extract file-level metadata using tools appropriate to the media types (e.g., MediaInfo, DROID, custom scripts). Compile this data into a structured inventory that reflects file counts, formats, codecs, durations, and folder structures. Identify errors or structural issues such as path length violations, nesting depth, and duplication patterns.

3. Define Representativity Criteria
Representativity is not a fixed metric—it is a context-sensitive judgment. For the Cossette collection, BAnQ had to determine what constituted a “representative” advertisement across variations in format, versioning, or completeness. Institutions should consider:

Breadth across campaigns or time periods

Presence of both full and excerpted versions

Technical quality and clarity

Evidence of production evolution or variation

4. Establish Appraisal and Triage Logic
Articulate the criteria by which files will be prioritized, retained, or excluded. This may involve threshold-based filters (e.g., discard files below a certain resolution or bitrate unless unique), clustering approaches to identify near-duplicates, or identification of unique markers such as logos, slates, or embedded timecodes. Develop logic that balances comprehensiveness with feasibility.

5. Apply Tools to Operationalize Retention Decisions
Implement scripts or manual review workflows that translate appraisal logic into action. This might include:

Flagging files with matching names but divergent specs

Grouping variants for manual selection of a “best” version

Identifying and isolating files that fall below retention thresholds

6. Ensure Traceability and Transparency
All decisions, especially those resulting in the exclusion or downranking of files, should be documented. Where possible, retain a shadow record of discarded files (e.g., filename, checksum, metadata snapshot) to preserve evidence of prior existence. Maintain linkages between retained digital surrogates and their original carriers or descriptive records.

This framework, while drawn from the specific circumstances of the Cossette fonds, can be adapted to other legacy digital collections—particularly those that arrive without consistent metadata, originate from decentralized workflows, or have undergone undocumented digitization processes. It is particularly useful when resource constraints preclude total retention and demand a more selective, evidence-based approach.

By surfacing the underlying logic of strategic retention—not just the outputs—it becomes possible to transform individual case responses into institutional memory, and institutional memory into fieldwide practice.


